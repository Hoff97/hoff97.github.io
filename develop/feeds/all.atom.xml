<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>haskai</title><link href="https://hoff97.github.io/develop/" rel="alternate"></link><link href="https://hoff97.github.io/develop/feeds/all.atom.xml" rel="self"></link><id>https://hoff97.github.io/develop/</id><updated>2021-09-16T12:09:00+02:00</updated><subtitle>Software Developer</subtitle><subtitle>Software Developer</subtitle><entry><title>TensorJS - Writing a fast deep learning library for the browser</title><link href="https://hoff97.github.io/develop/tensorjs.html" rel="alternate"></link><published>2021-09-16T12:09:00+02:00</published><updated>2021-09-16T12:09:00+02:00</updated><author><name>Frithjof Winkelmann</name></author><id>tag:hoff97.github.io,2021-09-16:/develop/tensorjs.html</id><summary type="html">&lt;p&gt;During the lockdown in spring of 2021 I wrote a deep learning library for the web. Here's what I learned.&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I build &lt;a href="https://detext.haskai.de/client/"&gt;Detext&lt;/a&gt;, I was looking for deep learning libraries for the browser.
The main 2 options where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/onnxjs"&gt;Onnx.JS&lt;/a&gt;, developed by Microsoft but not actively maintained at the time&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/js"&gt;TensorFlow.js&lt;/a&gt;, which is pretty mature and well maintained by facebook, but uses its own storage format for
  trained networks. Converting a trained Pytorch model to this format proved to be very challenging at the time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, what do you do when there's no viable inference library for executing your
PyTorch/ONNX model? You write your own library of course.
The requirements that I had for this library specifically where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast execution of reasonably sized models on most hardware and browsers. For me, reasonably
  sized meant a MobileNet on the harware that I had access to - a pretty shitty
  laptop and a Okish mobile phone.&lt;/li&gt;
&lt;li&gt;A "low level" PyTorch like interface for doing computations with tensors - eg.
  being able to do something like
    &lt;code&gt;const a = new Tensor([1,2,3,4,5,6], {shape: [2,3]}); const b = new Tensor([1,2,3,4,5,6], {shape: [3,2]}); a.matmul(b);&lt;/code&gt;
  I wanted to have this, so that the pre- and postprocessing that most models
  typically need could be done as fast as executing the model itself.&lt;/li&gt;
&lt;li&gt;A "high level" interface for executing models stored in the ONNX format.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Tensors and how they are layed out in memory&lt;/h1&gt;
&lt;p&gt;Deep learning frameworks are centered around the concept of a tensor - which to us computer programmers is
basically a multidimensional array. Consider a multi dimensional array/tensor like the following one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[[[1,2,3,4],
  [5,6,7,8],
  [9,10,11,12]],

 [[13,14,15,16],
  [17,18,19,20],
  [21,22,23,24]]]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This tensor has 3 dimensions, which we call its "rank". The dimensions have a length of
&lt;code&gt;[2,3,4]&lt;/code&gt; respectively, which we summarize as the tensors shape. The simplest layout
to store tensors in memory is called the contiguous layout - which basically
just stores the values in the order that you specify them, together with the shape of the tensor.&lt;/p&gt;
&lt;p&gt;This layout allows you to have zero-copy reshapes of tensors, but not much else.
While there is also the strided layout that allows you to have zero copy
transpose and range selection operations, I chose to go with the contiguous layout,
since its easy to implement.&lt;/p&gt;
&lt;h1&gt;Execution backends on the Web&lt;/h1&gt;
&lt;p&gt;While tensor operations are very straightforward to implement in Javascript, these implementations
will not be fast enough to execute any reasonably sized model.
Luckily there are other options for fast code execution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://webassembly.org/"&gt;Webassembly&lt;/a&gt;, a binary instruction format that can give you faster runtime than simple 
  Javascript - if optimized sufficiently.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/WebGL"&gt;WebGL&lt;/a&gt;, a Javascript API intended for 2D and 3D rendering.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Webassembly&lt;/h2&gt;
&lt;p&gt;Webassembly (WASM) by now is supported by all major browsers. While you can in principle handwrite Webassembly
in the &lt;a href="https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format"&gt;WAT format&lt;/a&gt;,
I chose to use Rust. Compiling to WASM is pretty well supported and
&lt;a href="https://github.com/rustwasm/wasm-bindgen"&gt;wasm-bindgen&lt;/a&gt; even generates the JS boilerplate code
for calling into WASM, and corresponding Typescript Type definitions for you.
With this writing the WASM backend was way simpler than expected.&lt;/p&gt;
&lt;h2&gt;WebGL&lt;/h2&gt;
&lt;p&gt;WebGL was originally intended for 2D and 3D visualizations accelerated by GPUs.
So how do you abuse this API to implement fast tensor operations?&lt;/p&gt;
&lt;p&gt;The traditional pipeline in WebGL looks a bit like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="WebGL standard pipeline" src="{filename}/../images/tensorjs/webgl_standard.png"&gt;&lt;/p&gt;
&lt;p&gt;You feed the geometry of the scene and textures to the vertex and fragment shaders.
The vertex shader determines the position of all vertices on the screen, while
the fragment shader determines the color of each pixel on the screen - typically
using the textures.&lt;/p&gt;
&lt;p&gt;By setting up a very simple scene geometry of a single rectengular plane
that fills up the whole screen, you can use the textures as
your input tensors and treat the rendered output as the result tensor.&lt;/p&gt;
&lt;p&gt;&lt;img alt="WebGL for GPGPU" src="{filename}/../images/tensorjs/webgl_gpgpu.png"&gt;&lt;/p&gt;
&lt;p&gt;Doing this allows you to implement many operations by simply writing the correct
fragment shader.&lt;/p&gt;
&lt;p&gt;There is one detail that I'm going over here - which is the conversion of tensors
to textures and back. For this you&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Represent the tensor in the contiguous layout&lt;/li&gt;
&lt;li&gt;Create a texture of fitting height and width. If the tensor has a size that is not conveniently 
   dividable into integer width and height you create a texture that is slightly bigger than needed.&lt;/li&gt;
&lt;li&gt;You fill the tensor data into the texture pixels row by row. Since each pixel consists of
   4 floating point values for the red, green, blue and transparent component, you only need
   1/4th of the pixels of the size of your tensor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the fragment shader you of course also have to do this conversion between tensor indices
and texture coordinates.&lt;/p&gt;
&lt;h3&gt;Considerations for fast speed&lt;/h3&gt;
&lt;p&gt;TODO: talk about precompiling shaders, keeping all data on GPU&lt;/p&gt;
&lt;h1&gt;Features of TensorJS&lt;/h1&gt;
&lt;h2&gt;ONNX support&lt;/h2&gt;
&lt;p&gt;TODO: List of supported operators&lt;/p&gt;
&lt;h2&gt;Automatic differentiation&lt;/h2&gt;
&lt;p&gt;TODO: Quickly talk about graph representation&lt;/p&gt;
&lt;h2&gt;Optimizers and models&lt;/h2&gt;
&lt;p&gt;TODO: How do optimizers discover model parameters&lt;/p&gt;
&lt;h2&gt;Sparse tensors&lt;/h2&gt;
&lt;p&gt;TODO: Short explanation of sparse tensors&lt;/p&gt;
&lt;h1&gt;Example applications&lt;/h1&gt;
&lt;p&gt;TODO: Mobilenet inference, style transfer, on device model finetuning&lt;/p&gt;
&lt;h1&gt;Possible extensions&lt;/h1&gt;</content><category term="Machine learning"></category><category term="deep-learning"></category><category term="javascript"></category></entry><entry><title>Learning Multi-view Camera Relocalization with Graph Neural Networks</title><link href="https://hoff97.github.io/develop/multi-view-reloc.html" rel="alternate"></link><published>2021-01-13T13:00:00+01:00</published><updated>2021-01-13T13:00:00+01:00</updated><author><name>Frithjof Winkelmann</name></author><id>tag:hoff97.github.io,2021-01-13:/develop/multi-view-reloc.html</id><summary type="html">&lt;p&gt;Paper analysis of "Learning Multi-view Camera Relocalization with Graph Neural Networks"&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;link rel="stylesheet"  href="{filename}../style.css"&gt;&lt;/p&gt;
&lt;p&gt;This blog post will review the paper
&lt;a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xue_Learning_Multi-View_Camera_Relocalization_With_Graph_Neural_Networks_CVPR_2020_paper.html"&gt;Learning Multi-view Camera Relocalization with Graph Neural Networks&lt;/a&gt;
presented at CVPR 2020. I will cover related literature,
explain the methodology and offer my own view at the end.&lt;/p&gt;
&lt;p&gt;Note: This blog post has several expandable sections. They are not required
for understanding the paper, but contain further information for
the interested reader.&lt;/p&gt;
&lt;h2&gt;Terminology&lt;/h2&gt;
&lt;p&gt;This section clarifies a few concepts that are needed to
understand camera re-localization. If you are already experienced
in computer vision you can probably skip it.&lt;/p&gt;
&lt;p&gt;&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;
&lt;details&gt;
  &lt;summary&gt;Poses&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;A pose describes the position and orientation of
  an object in 3D space. It consists of a translational
  component &lt;span class="math"&gt;\(\vec{t} \in \mathbb{R}^3\)&lt;/span&gt; and a orientational
  component, often represented as a quaternion &lt;span class="math"&gt;\(q \in \mathbb{H}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A 3D pose can also be described by a matrix
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  p \in \mathbb{R}^{4 \times 4} = \begin{pmatrix} R &amp;amp; \vec{t} \\ \vec{0} &amp;amp; 1\end{pmatrix}
  $$&lt;/div&gt;
&lt;p&gt;
  where &lt;span class="math"&gt;\(R \in \mathbb{R}^{3 \times 3}\)&lt;/span&gt; is a rotation matrix (so &lt;span class="math"&gt;\(det\ R = 1\)&lt;/span&gt;)
  and &lt;span class="math"&gt;\(\vec{t}\)&lt;/span&gt; is the translation vector.&lt;/p&gt;
&lt;p&gt;The set of all possible matrices representing a valid pose in
  3D is often written &lt;span class="math"&gt;\(SE(3)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These matrices act on vectors in homogeneous coordinates, where we write
  a position in 3D space &lt;span class="math"&gt;\(x \in \mathbb{R}^3\)&lt;/span&gt; as a 4 dimensional vector
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  \bar{x} = \begin{pmatrix}
    x_1 \\ x_2 \\ x_3 \\ 1
  \end{pmatrix}
  $$&lt;/div&gt;
&lt;p&gt;
  This allows the translation to be expressed in the matrix as well (while
  in non-homogeneous coordinates it can not be expressed in a matrix since it is not linear).&lt;/p&gt;
&lt;p&gt;&lt;/details&gt;
&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Camera Re-localization&lt;/h2&gt;
&lt;p&gt;Camera re-localization is the of task finding your position
in a known environment only by observation through pictures.
Humans do this constantly - when you get out of the subway
and want to find the exit thats closest to your destination
or when you make your daily way to work from memory.&lt;/p&gt;
&lt;p&gt;This is also a fundamental step in many computer vision tasks.
To be more specific, camera re-localization is the task
of finding the absolute pose &lt;span class="math"&gt;\(p_q \in SE(3)\)&lt;/span&gt; of a query image
&lt;span class="math"&gt;\(i_q \in \mathbb{R}^{3 \times W \times H}\)&lt;/span&gt; (with width &lt;span class="math"&gt;\(W\)&lt;/span&gt; and
height &lt;span class="math"&gt;\(H\)&lt;/span&gt;) given a &lt;strong&gt;database&lt;/strong&gt; of images &lt;span class="math"&gt;\(I = \{i_1, ..., i_n\}\)&lt;/span&gt;
of which the poses &lt;span class="math"&gt;\(P = \{p_1,...,p_n\}\)&lt;/span&gt; are already known.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Example of camera relocalization" src="{filename}/../images/multi_view_reloc/kings_position_localized.png"&gt;&lt;/p&gt;
&lt;p&gt;This can be seen in the above image. Given the positions of the 3
images at the top, we re-localize the image at the bottom at
the red camera position.&lt;/p&gt;
&lt;p&gt;&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;
&lt;details&gt;
  &lt;summary&gt;Applications&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;Where is this actually needed? As mentioned above, this
  is an important step in many computer vision pipelines,
  such as
  - Structure from Motion (SfM)
  - Simultaneous Localization and Mapping (SLAM)&lt;/p&gt;
&lt;p&gt;In structure from motion, a 3D model of the environment is recovered given
  only a sequence of images. This works in 4 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization:&lt;/strong&gt; 2 images are used to initialize the scene, by finding
     the relative pose of their cameras and detecting important keypoints in
     both images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Camera localization:&lt;/strong&gt; A new image is added to the scene using the
     previously located cameras and their keypoints. This can be viewed as
     a camera relocalization problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keypoint localization:&lt;/strong&gt; New keypoints from the just added image
     are localized in the scene.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bundle adjustment:&lt;/strong&gt; The previous steps often include small errors that
     would accumulate over time. Bundle adjustment minimizes those errors.
     After that the algorithm proceeds with step 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simultaneous Localization and Mapping can be viewed as SfM in the real time
  setting on a robot. This prohibits the usage of a bundle adjustment on the whole
  scene, since this would typically take too long. Instead only certain
  keyframes are saved. To still allow correcting accumulated errors,
  SLAM algorithms try to detect loop closures, where the robot returns to
  the same location. This can also be viewed as a camera relocalization
  problem.&lt;/p&gt;
&lt;p&gt;&lt;/details&gt;
&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Classical approaches&lt;/h2&gt;
&lt;p&gt;Given that this is part of long standing problems in
computer vision, there are of course many approaches
to solving this without deep learning.&lt;/p&gt;
&lt;p&gt;One common pipeline for this is divided into 3 steps:&lt;/p&gt;
&lt;h3&gt;1. Compute image keypoints of the query image&lt;/h3&gt;
&lt;p&gt;An example of the first step can be seen in the following picture.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ORB keypoints on a example query image" src="{filename}/../images/multi_view_reloc/orb.png"&gt;&lt;/p&gt;
&lt;p&gt;Image keypoints are interesting points in the image, such as
edges or corners. For each keypoint typically a
keypoint descriptor is computed - a feature that uniquely
describes the keypoint. There are many methods for doing this,
for example SIFT, SURF &lt;a href='#10.1007/978-3-642-15561-1_56' id='ref-10.1007/978-3-642-15561-1_56-1'&gt;(Calonder et al., 2010)&lt;/a&gt; or ORB &lt;a href='#6126544' id='ref-6126544-1'&gt;(Rublee et al., 2011)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;2. Find similar images in the image database&lt;/h3&gt;
&lt;p&gt;The keypoints and their descriptors are combined
into an overall image descriptor - which often is a vector
&lt;span class="math"&gt;\(d \in \mathbb{R}^D\)&lt;/span&gt;. This vector can be compared to the
image descriptor of images from the database. Images from similar
locations will have a similar vector and can thus be found
by nearest neighbor search.&lt;/p&gt;
&lt;p&gt;Approaches for computing image descriptors include Bag of Words &lt;a href='#GalvezLopez2012' id='ref-GalvezLopez2012-1'&gt;(GalvezLópez and Tardos, 2012)&lt;/a&gt;
 and Gist features &lt;a href='#Oliva2006BuildingTG' id='ref-Oliva2006BuildingTG-1'&gt;(Oliva and Torralba, 2006)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;3. Estimate the relative pose given one or many similar images from the database&lt;/h3&gt;
&lt;p&gt;When we have found on ore more similar images &lt;span class="math"&gt;\(i_d\)&lt;/span&gt; in the image
database, we can use their already known poses &lt;span class="math"&gt;\(p_d\)&lt;/span&gt; to estimate
the pose of the query image.&lt;/p&gt;
&lt;p&gt;For this we can use the keypoints from step 1, by computing
keypoint correspondences, like in the following picture.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ORB keypoints on a example query image" src="{filename}/../images/multi_view_reloc/orb_matched.png"&gt;&lt;/p&gt;
&lt;p&gt;These keypoint correspondences can then be used to estimate the
relative pose between the database image &lt;span class="math"&gt;\(i_d\)&lt;/span&gt; and the query image
&lt;span class="math"&gt;\(i_q\)&lt;/span&gt;, for example with the 5-point algorithm &lt;a href='#1288525' id='ref-1288525-1'&gt;(Nister, 2004)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;From classical to deep learning approaches&lt;/h3&gt;
&lt;p&gt;These steps are quite complex. Moreover many of these
approaches have difficulties with changing
illuminations and weather conditions, are quite
memory intensive and slow and require intrinsic calibration
to work right. This is why there is research
interest in replacing them with "simpler" deep learning approaches.&lt;/p&gt;
&lt;h2&gt;Deep learning approaches&lt;/h2&gt;
&lt;p&gt;Deep learning approaches for camera re-localization broadly
fall into two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approaches replacing step 1 and 2. These use a deep convolutional network to
  compute a feature vector for the query image that can be used to
  find similar images in the camera database. On example for this
  is NetVLAD &lt;a href='#DBLP:journals/corr/ArandjelovicGTP15' id='ref-DBLP:journals/corr/ArandjelovicGTP15-1'&gt;(Arandjelovic et al., 2015)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Approaches replacing all three steps with a deep neural network.
  They take one or multiple query images, pass them through a
  network and output a global pose for each of them. Since the
  network is optimized on the predicted poses this is called
  &lt;strong&gt;abolute pose regression&lt;/strong&gt;. The paper that sparked of this
  line of research is called PoseNet &lt;a href='#kendall2015convolutional' id='ref-kendall2015convolutional-1'&gt;(Kendall et al., 2015)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;PoseNet&lt;/h3&gt;
&lt;p&gt;PoseNet is a deep convolutional network, namely GoogLeNet &lt;a href='#DBLP:journals/corr/SzegedyLJSRAEVR14' id='ref-DBLP:journals/corr/SzegedyLJSRAEVR14-1'&gt;(Szegedy et al., 2014)&lt;/a&gt;, trained
on predicting a single pose &lt;span class="math"&gt;\(p_q\)&lt;/span&gt; for an input query image &lt;span class="math"&gt;\(i_q\)&lt;/span&gt;.
It is trained on scenes of the 7Scenes &lt;a href='#glocker2013real-time' id='ref-glocker2013real-time-1'&gt;(Glocker et al., 2013)&lt;/a&gt; and
Cambridge Landmarks &lt;a href='#kendall2015convolutional' id='ref-kendall2015convolutional-2'&gt;(Kendall et al., 2015)&lt;/a&gt; datasets. Each scene
contains one or multiple training and testing sequences.
It is important to mention that PoseNet, like &lt;strong&gt;all&lt;/strong&gt; absolute
pose regression approaches, works for a single scene only
and has to be retrained from scratch to work on other scenes.&lt;/p&gt;
&lt;p&gt;While PoseNet archieves "only" average accuracy (around 2m, 6°
accuracy), it runs very fast at ~5ms per picture, which is quite
impressive.&lt;/p&gt;
&lt;p&gt;There are several improvements to PoseNet, which include MapNet &lt;a href='#DBLP:journals/corr/abs-1712-03342' id='ref-DBLP:journals/corr/abs-1712-03342-1'&gt;(Brahmbhatt et al., 2017)&lt;/a&gt;,
VidLoc &lt;a href='#DBLP:journals/corr/ClarkWMTW17' id='ref-DBLP:journals/corr/ClarkWMTW17-1'&gt;(Clark et al., 2017)&lt;/a&gt; and VlocNet &lt;a href='#DBLP:journals/corr/abs-1803-03642' id='ref-DBLP:journals/corr/abs-1803-03642-1'&gt;(Valada et al., 2018)&lt;/a&gt; &lt;a href='#DBLP:journals/corr/abs-1804-08366' id='ref-DBLP:journals/corr/abs-1804-08366-1'&gt;(Radwan et al., 2018)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;VidLoc&lt;/h3&gt;
&lt;p&gt;The basic idea of VidLoc is to not re-localize single images
but instead a whole image sequence &lt;span class="math"&gt;\(i_1,...,i_n\)&lt;/span&gt; (a video - hence the name)
at once. To do this, the images in the sequence are first preprocessed
by some convolutional layers. The resulting feature maps are then passed through several
bi-directional LSTM layers, which in the end output
the sequence of poses &lt;span class="math"&gt;\(p_1,...,p_n\)&lt;/span&gt; for the images.&lt;/p&gt;
&lt;p&gt;The overall network architecture can be seen in the following figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Architecture of VidLoc" src="{filename}/../images/multi_view_reloc/vidloc.png"&gt;&lt;/p&gt;
&lt;p&gt;By taking advantage of the sequential nature of the input, VidLoc
can reach higher accuracy than PoseNet. The presented paper builds up on this.&lt;/p&gt;
&lt;h2&gt;Learning Multi-view Camera Relocalization with Graph Neural Networks&lt;/h2&gt;
&lt;p&gt;The main idea of this paper is to use Graph Neural Networks
instead of LSTMs to re-localize a sequence of images.
They propose a few smart approaches to learn the camera
poses in a graph based manner, which I will explain in the following.&lt;/p&gt;
&lt;h3&gt;Graph Neural Networks&lt;/h3&gt;
&lt;p&gt;Graph neural networks (GNNs) operate on graphs.
Given a graph &lt;span class="math"&gt;\(G=(V,E)\)&lt;/span&gt; with a vertex set &lt;span class="math"&gt;\(V=\{v_1,...,v_n\}\)&lt;/span&gt;
and edges &lt;span class="math"&gt;\(E=\{(v_i,v_j),...\}\)&lt;/span&gt; and some node
features &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; for each node &lt;span class="math"&gt;\(v_i\)&lt;/span&gt;, they define a function
&lt;span class="math"&gt;\(G' = F(G)\)&lt;/span&gt; operating on that graph.&lt;/p&gt;
&lt;p&gt;While they can in general change the graph structure (so add and
remove edges or nodes) we are going to assume that the graph
structure stays mostly unchanged. We are only interested in
mapping the input node features &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; to output node features &lt;span class="math"&gt;\(y_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is done with a mechanism called &lt;strong&gt;message passing&lt;/strong&gt;, which consists of
three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each edge in the graph we compute a message
   &lt;span class="math"&gt;\(m_{j \rightarrow i} = f_{message}(x_j, x_i)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We aggregate the messages arriving at one node. For this we first calculate
   an attention
   &lt;span class="math"&gt;\(a_{j \rightarrow i} = f_{attention}(x_j, x_i)\)&lt;/span&gt; and use this to sum over the messages:
   &lt;span class="math"&gt;\(m_i = \tfrac{1}{|N(i)|} \sum_{j \in N(i)} a_{j \rightarrow i} * m_{j \rightarrow i}\)&lt;/span&gt;.
   It should be noted that not all message passing networks rely on an attention
   calulation. This is a deliberate design choice in this paper.&lt;/li&gt;
&lt;li&gt;The message is used to update the node feature:
   &lt;span class="math"&gt;\(x_i' = f_{update}(m_i,x_i)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The functions &lt;span class="math"&gt;\(f_{message}\)&lt;/span&gt;, &lt;span class="math"&gt;\(f_{update}\)&lt;/span&gt; are represented by some neural network. Normally
the node features are vectors in &lt;span class="math"&gt;\(\mathbb{R}^D\)&lt;/span&gt;, in this case these function
can be represented by a couple of fully connected layers.
GNNs typically chain a couple of these message passing layers, to allow
representing more complex functions.&lt;/p&gt;
&lt;h3&gt;Application to image sequences&lt;/h3&gt;
&lt;p&gt;Now the question is how this is used to relocate a sequence of images
&lt;span class="math"&gt;\(i_1,...,i_n \in \mathbb{R}^{3 \times H \times W}\)&lt;/span&gt;.
First, the images are passed through some convolutional layers yielding
feature maps &lt;span class="math"&gt;\(i'_j \in \mathbb{R}^{C \times H' \times W'}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These feature maps are the node features of the graph. Now we only have to define the edges of the graph.
For this the authors propose to simply initialize with the fully connected graph, that is connect every node to every other node.&lt;/p&gt;
&lt;p&gt;Since the node features are image feature maps, the functions &lt;span class="math"&gt;\(f_{message}\)&lt;/span&gt; and &lt;span class="math"&gt;\(f_{update}\)&lt;/span&gt; are defined by
two convolutional layers.&lt;/p&gt;
&lt;p&gt;&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;
&lt;details&gt;
  &lt;summary&gt;Attention calculation&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;If you wonder how the function &lt;span class="math"&gt;\(f_{attention}\)&lt;/span&gt; is defined:&lt;/p&gt;
&lt;div class="math"&gt;$$
    f_{attention}(x_i, x_j) = \sigma(cs(vec(x_i), vec(x_j)))
  $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(vec(\circ)\)&lt;/span&gt; flattens the input tensor to a vector
  and &lt;span class="math"&gt;\(cs(\circ, \circ)\)&lt;/span&gt; is the cosine similarity.
&lt;/details&gt;
&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;&lt;/p&gt;
&lt;h3&gt;Edge pooling&lt;/h3&gt;
&lt;p&gt;The computational cost for computing messages on the fully connected graph with &lt;span class="math"&gt;\(n\)&lt;/span&gt; nodes
is &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt;. For this reason the edges are pruned in the later message passing layers
to make the graph more sparse. For this the correlation between connected nodes is computed as&lt;/p&gt;
&lt;div class="math"&gt;$$
c_{j \rightarrow i} = cs(maxpool(x_i),maxpool(x_j))
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(cs(\circ, \circ)\)&lt;/span&gt; is the cosine similarity and the maxpooling is done over the
spatial dimensions of the node features.&lt;/p&gt;
&lt;p&gt;The authors argue that this not only reduces the computational cost, but also
reduces redundancy and prevents overfitting.&lt;/p&gt;
&lt;h3&gt;Overall network architecture&lt;/h3&gt;
&lt;p&gt;With this knowledge we are ready to look at the overall network architecture.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overall network architecture" src="{filename}/../images/multi_view_reloc/details_img2.png"&gt;&lt;/p&gt;
&lt;p&gt;The red nodes are convolutional layers, the green nodes message passing layers.
The result of all message passing layers is concatenated along the channel
dimension and then pooled across the spatial dimensions. Two fully connected
layers predict the position &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; and orientation &lt;span class="math"&gt;\(q_i\)&lt;/span&gt; (as a quaternion)
of each input image.&lt;/p&gt;
&lt;h3&gt;Loss function&lt;/h3&gt;
&lt;p&gt;Given the predicted &lt;span class="math"&gt;\(\hat{p}_i = (\hat{t}_i, \hat{q}_i)\)&lt;/span&gt; and
ground truth pose &lt;span class="math"&gt;\(p_i = (t_i, q_i)\)&lt;/span&gt;, the loss for one image is defined as&lt;/p&gt;
&lt;div class="math"&gt;$$
d(p_i, \hat{p}_i) = ||t_i - \hat{t}_i||_1*e^{-\beta_p} + \beta_p +
   ||r_i - \hat{r}_i||_1*e^{-\gamma_p} + \gamma_p
$$&lt;/div&gt;
&lt;p&gt;which was first proposed in &lt;a href='#Kendall2017GeometricLF' id='ref-Kendall2017GeometricLF-1'&gt;(Kendall and Cipolla, 2017)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\beta_p\)&lt;/span&gt;, &lt;span class="math"&gt;\(\gamma_p\)&lt;/span&gt; are parameters that are optimized jointly
with the network parameters and automatically balance the rotation and
translation error against each other.&lt;/p&gt;
&lt;p&gt;&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;
&lt;details&gt;
  &lt;summary&gt;We can derive where this loss comes from in more detailed manner.
  If you want to see that click here - it is not necessary for understanding the
  paper though.&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;Normally when predicting samples &lt;span class="math"&gt;\(r_i, t_i\)&lt;/span&gt;, we just minimize
  the L2 loss: &lt;span class="math"&gt;\(L(p_i,\hat{p}_i) = ||r_i - \hat{r}_i||_2 + ||t_i - \hat{t}_i||_2\)&lt;/span&gt;.
  This corresponds to optimizing the negative log likelihood &lt;span class="math"&gt;\(-log(P(r_i,t_i|x_i;W))\)&lt;/span&gt;
  (where &lt;span class="math"&gt;\(W\)&lt;/span&gt; are the model parameters), assuming that all samples are normally
  distributed, eg:
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  P(r_i,t_i|x_i;W) = N(\hat{r}_i|r_i;1) * N(\hat{t}_i|t_i;1)
  $$&lt;/div&gt;
&lt;p&gt;The notation &lt;span class="math"&gt;\(N(x|\mu,\sigma^2)\)&lt;/span&gt; refers to the likelihood of &lt;span class="math"&gt;\(x\)&lt;/span&gt; when
  drawn from a normal distribution with mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But this has two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It assumes that the uncertainty is the same for the translation &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; and rotation &lt;span class="math"&gt;\(r_i\)&lt;/span&gt;
     This is not the case, since they are measured in different units.&lt;/li&gt;
&lt;li&gt;It assumes that the uncertainty is the same across all samples.
     This isn't necessarily true either, since there might be samples where estimating
     the position is very easy (and hence has low uncertainty), and others where
     this is a lot harder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To solve this problem, a per-sample uncertainty
  &lt;span class="math"&gt;\(\sigma_{r_i}^2\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma_{t_i}^2\)&lt;/span&gt; is introduced.
  Of course this uncertainty is not part of the ground truth data. For this
  reason it has to be optimized together with the model parameters.&lt;/p&gt;
&lt;p&gt;With this the negative log-likelihood becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$
   -log(P(r_i,t_i|x_i;W)) = -log(N(\hat{r}_i|r_i;\sigma_{r_i}^2)) - log(N(\hat{t}_i|t_i;\sigma_{t_i}^2))\\
     \propto \tfrac{1}{\sigma_{r_i}^2} L(\hat{r}_i,r_i) + log(\sigma_{r_i}^2) +
     \tfrac{1}{\sigma_{t_i}^2} L(\hat{t}_i,t_i) + log(\sigma_{t_i}^2)
  $$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L(\hat{x},x)\)&lt;/span&gt; is the L2 distance. In &lt;a href='#Kendall2017GeometricLF' id='ref-Kendall2017GeometricLF-2'&gt;(Kendall and Cipolla, 2017)&lt;/a&gt; the authors
  found that using a L1 loss results in a higher accuracy.&lt;/p&gt;
&lt;p&gt;We just substitute &lt;span class="math"&gt;\(\beta_p = log(\sigma_{r_i}^2)\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(\gamma_p = log(\sigma_{t_i}^2)\)&lt;/span&gt;, which results in a numerically stable
  optimization. With this we get the loss as defined above.&lt;/p&gt;
&lt;p&gt;&lt;/details&gt;
&lt;hr style="border-top: 1px solid #BBB; margin: 5px;"/&gt;&lt;/p&gt;
&lt;p&gt;Additionally to this, the authors propose a graph based loss that
is defined on the relative poses &lt;span class="math"&gt;\(\omega_{ij}\)&lt;/span&gt; of the edges &lt;span class="math"&gt;\(e_{ij}\)&lt;/span&gt; after the
last edge pooling layer and results in the total loss of&lt;/p&gt;
&lt;div class="math"&gt;$$
 L = \tfrac{1}{|V|} \sum_{v_i \in V} d(p_i,\hat{p}_i) +
  \tfrac{1}{|E|} \sum_{(i,j) \in E} d(\omega_{ij},\hat{\omega}_{ij})
$$&lt;/div&gt;
&lt;p&gt;The authors argue that this graph based loss induces constraints similar
to those in a pose graph in SfM and SLAM applications.&lt;/p&gt;
&lt;h3&gt;Experiments&lt;/h3&gt;
&lt;p&gt;The authors evaluate their approach on the 7Scenes &lt;a href='#glocker2013real-time' id='ref-glocker2013real-time-2'&gt;(Glocker et al., 2013)&lt;/a&gt;, Cambridge &lt;a href='#kendall2015convolutional' id='ref-kendall2015convolutional-3'&gt;(Kendall et al., 2015)&lt;/a&gt; and Oxford
RobotCar datasets &lt;a href='#Maddern2016' id='ref-Maddern2016-1'&gt;(Maddern et al., 2016)&lt;/a&gt;.
Each of them contains training and testing sequences for several distinct
scenes. The model is trained on one scene at a time and then evaluated
on the same scene. For evaluation the translational error
&lt;span class="math"&gt;\(||t_i - \hat{t}_i||_2\)&lt;/span&gt; and rotational error &lt;span class="math"&gt;\(angle(q_i,\hat{q}_i)\)&lt;/span&gt; is
computed for each image (where &lt;span class="math"&gt;\(angle(\circ,\circ)\)&lt;/span&gt; gives the angle between two
quaternions) and the median is reported.&lt;/p&gt;
&lt;p&gt;The following table shows an excerpt of the results on the 7Scenes dataset,
for all results please check the paper.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method/Scene&lt;/th&gt;
&lt;th&gt;Chess&lt;/th&gt;
&lt;th&gt;Fire&lt;/th&gt;
&lt;th&gt;Heads&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PoseNet&lt;/td&gt;
&lt;td&gt;0.32m, 8.12°&lt;/td&gt;
&lt;td&gt;0.47m, 14.4°&lt;/td&gt;
&lt;td&gt;0.29m, 12.0°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VidLoc&lt;/td&gt;
&lt;td&gt;0.18m, -&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.26m&lt;/strong&gt;, -&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.14m&lt;/strong&gt;, -&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MapNet&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.08m&lt;/strong&gt;, 3.25°&lt;/td&gt;
&lt;td&gt;0.27m, 11.69°&lt;/td&gt;
&lt;td&gt;0.18m, 13.25°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Graph based&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.08m&lt;/strong&gt;, &lt;strong&gt;2.82°&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.26m&lt;/strong&gt;, &lt;strong&gt;8.94°&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.16m, &lt;strong&gt;11.41°&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The gist of the results is that the presented methods often yields SOTA accuracy
or at least very close to it. One can also plot the predicted trajectories
on the Oxford Robot Car dataset, which can be seen in the next picture.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overall network architecture" src="{filename}/../images/multi_view_reloc/result_vis_oxford.png"&gt;&lt;/p&gt;
&lt;p&gt;The black line shows the ground truth, while the red line shows the prediction
of the respective model. We can see that the predictions of the proposed
methods are superior in most cases.&lt;/p&gt;
&lt;h3&gt;Ablation study&lt;/h3&gt;
&lt;p&gt;The authors also do an ablation study on some components of their network
(on the Oxford Robot Car dataset).&lt;/p&gt;
&lt;p&gt;First they study what happens when they drop the graph based loss
in their optimization, so optimizing&lt;/p&gt;
&lt;div class="math"&gt;$$
 L = \tfrac{1}{|V|} \sum_{v_i \in V} d(p_i,\hat{p}_i)
$$&lt;/div&gt;
&lt;p&gt;instead of&lt;/p&gt;
&lt;div class="math"&gt;$$
 L = \tfrac{1}{|V|} \sum_{v_i \in V} d(p_i,\hat{p}_i) +
  \tfrac{1}{|E|} \sum_{(i,j) \in E} d(\omega_{ij},\hat{\omega}_{ij})
$$&lt;/div&gt;
&lt;p&gt;resulting in the following performance:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method/Scene&lt;/th&gt;
&lt;th&gt;LOOP1&lt;/th&gt;
&lt;th&gt;LOOP2&lt;/th&gt;
&lt;th&gt;FULL1&lt;/th&gt;
&lt;th&gt;FULL2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Without graph loss&lt;/td&gt;
&lt;td&gt;10.60m, 4.54°&lt;/td&gt;
&lt;td&gt;10.77m, 4.12°&lt;/td&gt;
&lt;td&gt;20.26m, 4.78°&lt;/td&gt;
&lt;td&gt;39.57m, 8.05°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With graph loss&lt;/td&gt;
&lt;td&gt;9.07m, 3.15°&lt;/td&gt;
&lt;td&gt;9.16m, 3.22°&lt;/td&gt;
&lt;td&gt;19.70m, 4.46°&lt;/td&gt;
&lt;td&gt;39.83m, 8.17°&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;we can see that the graph based loss has a positive impact
on accuracy in most cases.&lt;/p&gt;
&lt;p&gt;Next they study the influence of the amout of message passing layers used in the network. They drop some (or all) of the
later layers.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method/Scene&lt;/th&gt;
&lt;th&gt;LOOP1&lt;/th&gt;
&lt;th&gt;LOOP2&lt;/th&gt;
&lt;th&gt;FULL1&lt;/th&gt;
&lt;th&gt;FULL2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;With 1 MP layer&lt;/td&gt;
&lt;td&gt;9.07m, 3.15°&lt;/td&gt;
&lt;td&gt;9.16m, 3.22°&lt;/td&gt;
&lt;td&gt;19.70m, 4.46°&lt;/td&gt;
&lt;td&gt;39.83m, 8.17°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With 2 MP layers&lt;/td&gt;
&lt;td&gt;8.49m, 3.11°&lt;/td&gt;
&lt;td&gt;8.62m, 3.19°&lt;/td&gt;
&lt;td&gt;18.76m, 4.35°&lt;/td&gt;
&lt;td&gt;38.76m, 9.41°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With 3 MP layers&lt;/td&gt;
&lt;td&gt;8.46m, 3.02°&lt;/td&gt;
&lt;td&gt;7.68m, 2.78°&lt;/td&gt;
&lt;td&gt;17.35m, 3.59°&lt;/td&gt;
&lt;td&gt;36.84m, 8.22°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With 4 MP layers&lt;/td&gt;
&lt;td&gt;7.76m, 2.54°&lt;/td&gt;
&lt;td&gt;8.15m, 2.57°&lt;/td&gt;
&lt;td&gt;17.35m, 3.47°&lt;/td&gt;
&lt;td&gt;37.81m, 7.55°&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;More layers also improve the accuracy.&lt;/p&gt;
&lt;p&gt;Third, they study what happens, if the &lt;span class="math"&gt;\(f_{message}\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f_{update}\)&lt;/span&gt; functions are defined by simple
MLPs instead of CNNs and operate on feature vectors instead of
images. Both models are trained with one message passing
layer and no graph based loss.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method/Scene&lt;/th&gt;
&lt;th&gt;LOOP1&lt;/th&gt;
&lt;th&gt;LOOP2&lt;/th&gt;
&lt;th&gt;FULL1&lt;/th&gt;
&lt;th&gt;FULL2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;With 1 MLP layer&lt;/td&gt;
&lt;td&gt;23.43m, 9.75°&lt;/td&gt;
&lt;td&gt;24.65m, 10.55°&lt;/td&gt;
&lt;td&gt;34.47m, 4.07°&lt;/td&gt;
&lt;td&gt;58.16m, 9.73°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With 1 CNN layer&lt;/td&gt;
&lt;td&gt;10.60m, 4.54°&lt;/td&gt;
&lt;td&gt;10.77m, 4.12°&lt;/td&gt;
&lt;td&gt;20.26m, 4.78°&lt;/td&gt;
&lt;td&gt;39.57m, 8.05°&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see that the combination of GNNs with CNNs brings by far
the greatest improvement.&lt;/p&gt;
&lt;p&gt;Interestingly they do not evaluate the effect of the edge
pooling layer. This would be interesting to see, since they claim that
edge pooling prevents overfitting. One could alternatively also make
the hypothesis that more edges would lead to a better accuracy
since the update steps in the GNN can take information from more neighbors
into account.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;The authors propose an interesting approach to camera re-localization.
There are some interesting follow-up research questions that arise from
this paper.&lt;/p&gt;
&lt;h3&gt;Network architecture&lt;/h3&gt;
&lt;p&gt;The proposed network architecture is invariant to the sequence
order of the input images. If you shuffle the images
around, the same fully connected graph will be initialized and since
the edge pruning only relies on the node features, the same edges
will be pruned as well.&lt;/p&gt;
&lt;p&gt;If you forget about the edge pruning for a second, the proposed architecture
is actually equivalent to a transformer. (Transformers are Graph Neural
Networks on fully connected graphs. If you want to know more,
have a look at &lt;a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/"&gt;this&lt;/a&gt;
blog post for example).&lt;/p&gt;
&lt;p&gt;From this two interesting questions arise&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Would this network benefit from the positional information when
  applied to image sequences? Given the correspondence to transformers,
  one could try to include a positional encoding &lt;a href='#DBLP:journals/corr/VaswaniSPUJGKP17' id='ref-DBLP:journals/corr/VaswaniSPUJGKP17-1'&gt;(Vaswani et al., 2017)&lt;/a&gt;
  in the input features.&lt;/li&gt;
&lt;li&gt;Does this network architecture work for unordered sequences as well? Many
  image collections in reconstruction &lt;a href='#romeday' id='ref-romeday-1'&gt;(Frahm et al., 2010)&lt;/a&gt; are mined
  from the internet and thus do not have a sequential nature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From question 2 we can derive another follow up question. Many
image collections mined from the internet consist of photos taken with
many different cameras and thus with many different distortions and
calibrations. It would be interesting to know if this method can
deal with that - especially since autocalibration is not an easy
problem for classical methods.&lt;/p&gt;
&lt;h3&gt;Runtime&lt;/h3&gt;
&lt;p&gt;The graph for the GNN is initialized fully connected. Thus it will contain
&lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt; edges for a sequence with &lt;span class="math"&gt;\(n\)&lt;/span&gt; images. This incurs a
space and time complexity of &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt; in the first message passing
layer. This is of course not nice for longer sequences. One could try to
replace the fully connected Graph attention layer with a sparse variant
similar to sparse transformers &lt;a href='#DBLP:journals/corr/abs-1904-10509' id='ref-DBLP:journals/corr/abs-1904-10509-1'&gt;(Child et al., 2019)&lt;/a&gt; &lt;a href='#choromanski2020rethinking' id='ref-choromanski2020rethinking-1'&gt;(Choromanski et al., 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;CamNet&lt;/h3&gt;
&lt;p&gt;The authors cite CamNet &lt;a href='#Ding_2019_ICCV' id='ref-Ding_2019_ICCV-1'&gt;(Ding et al., 2019)&lt;/a&gt;, another deep learning based method
for camera re-localization (although it is not an Absolute Pose Regression
method). They do &lt;strong&gt;not&lt;/strong&gt; include it in their evaluation. The table below
comparse the performance of CamNet and GNN based method on the 7Scenes
dataset:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method/Scene&lt;/th&gt;
&lt;th&gt;Chess&lt;/th&gt;
&lt;th&gt;Fire&lt;/th&gt;
&lt;th&gt;Heads&lt;/th&gt;
&lt;th&gt;Office&lt;/th&gt;
&lt;th&gt;Pumpkin&lt;/th&gt;
&lt;th&gt;Kitchen&lt;/th&gt;
&lt;th&gt;Stairs&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Graph based NN&lt;/td&gt;
&lt;td&gt;0.08m, 2.82°&lt;/td&gt;
&lt;td&gt;0.26m, 8.94°&lt;/td&gt;
&lt;td&gt;0.17m, 11.41°&lt;/td&gt;
&lt;td&gt;0.18m, 5.08°&lt;/td&gt;
&lt;td&gt;0.15m, 2.77°&lt;/td&gt;
&lt;td&gt;0.25m, 4.48°&lt;/td&gt;
&lt;td&gt;0.23m, 8.78°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CamNet&lt;/td&gt;
&lt;td&gt;0.04m, 1.73°&lt;/td&gt;
&lt;td&gt;0.03m, 1.74°&lt;/td&gt;
&lt;td&gt;0.05m, 1.98°&lt;/td&gt;
&lt;td&gt;0.04m, 1.62°&lt;/td&gt;
&lt;td&gt;0.04m, 1.64°&lt;/td&gt;
&lt;td&gt;0.04m, 1.63°&lt;/td&gt;
&lt;td&gt;0.04m, 1.51°&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can also have a look at the visualization on the Oxford Robot Car
dataset (only LOOP1 since CamNet doesn't offer any other visualizations).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visual comparison of the results to CamNet" src="{filename}/../images/multi_view_reloc/comparison_camnet.png"&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see that CamNet outperforms the graph based network.&lt;/p&gt;
&lt;p&gt;Citing the paper show that the authors knew of this approach, yet they didn't
include them in their final results.&lt;/p&gt;
&lt;h3&gt;Other approaches&lt;/h3&gt;
&lt;p&gt;The last section already showed that there are deep learning methods that do
not do absolute pose regression but perform better for camera re-localization.
There is even research &lt;a href='#DBLP:journals/corr/abs-1903-07504' id='ref-DBLP:journals/corr/abs-1903-07504-1'&gt;(Sattler et al., 2019)&lt;/a&gt; that suggests that absolute pose regression
can &lt;strong&gt;not&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generalize beyond the training data.&lt;/li&gt;
&lt;li&gt;Perform better than structured approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is motivated by the fact that these methods, while relatively fast
have never outperformed classical approaches for camera re-localization &lt;a href='#DBLP:journals/corr/abs-1903-07504' id='ref-DBLP:journals/corr/abs-1903-07504-2'&gt;(Sattler et al., 2019)&lt;/a&gt;.
This does not mean that this line of research is completely useless though.&lt;/p&gt;
&lt;p&gt;While absolute pose regression might not necessarily be the best application
for end-to-end deep learning - there are other directions where it might
be feasible to integrate deep learning.&lt;/p&gt;
&lt;p&gt;Graph based methods especially would be an interesting application for
Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM).
There is some research on applying deep learning to Visual Odometry &lt;a href='#DBLP:journals/corr/abs-1709-08429' id='ref-DBLP:journals/corr/abs-1709-08429-1'&gt;(Wang et al., 2017)&lt;/a&gt;
Sfm &lt;a href='#wei2020deepsfm' id='ref-wei2020deepsfm-1'&gt;(Wei et al., 2020)&lt;/a&gt;
and SLAM &lt;a href='#DBLP:journals/corr/abs-1901-07223' id='ref-DBLP:journals/corr/abs-1901-07223-1'&gt;(Kang et al., 2019)&lt;/a&gt;. GNNs for these topics have only been evaluated
in small use cases &lt;a href='#DBLP:journals/corr/abs-1802-06857' id='ref-DBLP:journals/corr/abs-1802-06857-1'&gt;(Parisotto et al., 2018)&lt;/a&gt; and are an interesting area of further research.&lt;/p&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;In the paper the authors claim that an implementation is provided at &lt;a href="https://github.com/feixue94/grnet"&gt;https://github.com/feixue94/grnet&lt;/a&gt;.
Unfortunately the github repository at that URL provides nothing but an empty Readme file.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='DBLP:journals/corr/ArandjelovicGTP15'&gt;Relja Arandjelovic, Petr Gron&lt;span class="bibtex-protected"&gt;&lt;span class="bibtex-protected"&gt;á&lt;/span&gt;&lt;/span&gt;t, Akihiko Torii, Tom&lt;span class="bibtex-protected"&gt;&lt;span class="bibtex-protected"&gt;á&lt;/span&gt;&lt;/span&gt;s Pajdla, and Josef Sivic.
Netvlad: &lt;span class="bibtex-protected"&gt;CNN&lt;/span&gt; architecture for weakly supervised place recognition.
&lt;em&gt;CoRR&lt;/em&gt;, 2015.
URL: &lt;a href="http://arxiv.org/abs/1511.07247"&gt;http://arxiv.org/abs/1511.07247&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1511.07247"&gt;arXiv:1511.07247&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/ArandjelovicGTP15-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1712-03342'&gt;Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, and Jan Kautz.
Mapnet: geometry-aware learning of maps for camera localization.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1712.03342"&gt;http://arxiv.org/abs/1712.03342&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1712.03342"&gt;arXiv:1712.03342&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1712-03342-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='10.1007/978-3-642-15561-1_56'&gt;Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua.
Brief: binary robust independent elementary features.
In Kostas Daniilidis, Petros Maragos, and Nikos Paragios, editors, &lt;em&gt;Computer Vision – ECCV 2010&lt;/em&gt;, 778–792. Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. &lt;a class="cite-backref" href="#ref-10.1007/978-3-642-15561-1_56-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1904-10509'&gt;Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers.
&lt;em&gt;CoRR&lt;/em&gt;, 2019.
URL: &lt;a href="http://arxiv.org/abs/1904.10509"&gt;http://arxiv.org/abs/1904.10509&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1904.10509"&gt;arXiv:1904.10509&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1904-10509-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='choromanski2020rethinking'&gt;Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller.
Rethinking attention with performers.
2020.
&lt;a href="https://arxiv.org/abs/2009.14794"&gt;arXiv:2009.14794&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-choromanski2020rethinking-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/ClarkWMTW17'&gt;Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, and Hongkai Wen.
Vidloc: 6-dof video-clip relocalization.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1702.06521"&gt;http://arxiv.org/abs/1702.06521&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1702.06521"&gt;arXiv:1702.06521&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/ClarkWMTW17-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Ding_2019_ICCV'&gt;Mingyu Ding, Zhe Wang, Jiankai Sun, Jianping Shi, and Ping Luo.
Camnet: coarse-to-fine retrieval for camera re-localization.
In &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)&lt;/em&gt;. October 2019. &lt;a class="cite-backref" href="#ref-Ding_2019_ICCV-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='romeday'&gt;Jan-Michael Frahm, Pierre Fite-Georgel, David Gallup, Tim Johnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen, Enrique Dunn, Brian Clipp, Svetlana Lazebnik, and Marc Pollefeys.
Building rome on a cloudless day.
In &lt;em&gt;Building Rome on a Cloudless Day&lt;/em&gt;, volume 6314, 368&amp;ndash;381. 07 2010.
&lt;a href="https://doi.org/10.1007/978-3-642-15561-1_27"&gt;doi:10.1007/978-3-642-15561-1_27&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-romeday-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='glocker2013real-time'&gt;Ben Glocker, Shahram Izadi, Jamie Shotton, and Antonio Criminisi.
Real-time rgb-d camera relocalization.
In &lt;em&gt;International Symposium on Mixed and Augmented Reality (ISMAR)&lt;/em&gt;. IEEE, October 2013.
URL: &lt;a href="https://www.microsoft.com/en-us/research/publication/real-time-rgb-d-camera-relocalization/"&gt;https://www.microsoft.com/en-us/research/publication/real-time-rgb-d-camera-relocalization/&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-glocker2013real-time-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-glocker2013real-time-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-glocker2013real-time-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1901-07223'&gt;Rong Kang, Jieqi Shi, Xueming Li, Yang Liu, and Xiao Liu.
&lt;span class="bibtex-protected"&gt;DF-SLAM:&lt;/span&gt; &lt;span class="bibtex-protected"&gt;A&lt;/span&gt; deep-learning enhanced visual &lt;span class="bibtex-protected"&gt;SLAM&lt;/span&gt; system based on deep local features.
&lt;em&gt;CoRR&lt;/em&gt;, 2019.
URL: &lt;a href="http://arxiv.org/abs/1901.07223"&gt;http://arxiv.org/abs/1901.07223&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1901.07223"&gt;arXiv:1901.07223&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1901-07223-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Kendall2017GeometricLF'&gt;Alex Kendall and R.&amp;nbsp;Cipolla.
Geometric loss functions for camera pose regression with deep learning.
&lt;em&gt;2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, pages 6555&amp;ndash;6564, 2017. &lt;a class="cite-backref" href="#ref-Kendall2017GeometricLF-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-Kendall2017GeometricLF-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-Kendall2017GeometricLF-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='kendall2015convolutional'&gt;Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Posenet: a convolutional network for real-time 6-dof camera relocalization.
2015. &lt;a class="cite-backref" href="#ref-kendall2015convolutional-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-kendall2015convolutional-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-kendall2015convolutional-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-kendall2015convolutional-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='Maddern2016'&gt;Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman.
1 year, 1000 km: the oxford robotcar dataset.
&lt;em&gt;The International Journal of Robotics Research&lt;/em&gt;, 36:, 11 2016.
&lt;a href="https://doi.org/10.1177/0278364916679498"&gt;doi:10.1177/0278364916679498&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Maddern2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Oliva2006BuildingTG'&gt;A.&amp;nbsp;Oliva and A.&amp;nbsp;Torralba.
Building the gist of a scene: the role of global image features in recognition.
&lt;em&gt;Progress in brain research&lt;/em&gt;, 155:23&amp;ndash;36, 2006. &lt;a class="cite-backref" href="#ref-Oliva2006BuildingTG-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1802-06857'&gt;Emilio Parisotto, Devendra&amp;nbsp;Singh Chaplot, Jian Zhang, and Ruslan Salakhutdinov.
Global pose estimation with an attention-based recurrent network.
&lt;em&gt;CoRR&lt;/em&gt;, 2018.
URL: &lt;a href="http://arxiv.org/abs/1802.06857"&gt;http://arxiv.org/abs/1802.06857&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1802.06857"&gt;arXiv:1802.06857&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1802-06857-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1804-08366'&gt;Noha Radwan, Abhinav Valada, and Wolfram Burgard.
Vlocnet++: deep multitask learning for semantic visual localization and odometry.
&lt;em&gt;CoRR&lt;/em&gt;, 2018.
URL: &lt;a href="http://arxiv.org/abs/1804.08366"&gt;http://arxiv.org/abs/1804.08366&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1804.08366"&gt;arXiv:1804.08366&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1804-08366-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1903-07504'&gt;Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal&lt;span class="bibtex-protected"&gt;-&lt;/span&gt;Taix&lt;span class="bibtex-protected"&gt;&lt;span class="bibtex-protected"&gt;é&lt;/span&gt;&lt;/span&gt;.
Understanding the limitations of cnn-based absolute camera pose regression.
&lt;em&gt;CoRR&lt;/em&gt;, 2019.
URL: &lt;a href="http://arxiv.org/abs/1903.07504"&gt;http://arxiv.org/abs/1903.07504&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1903.07504"&gt;arXiv:1903.07504&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1903-07504-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1903-07504-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1903-07504-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/SzegedyLJSRAEVR14'&gt;Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott&amp;nbsp;E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
Going deeper with convolutions.
&lt;em&gt;CoRR&lt;/em&gt;, 2014.
URL: &lt;a href="http://arxiv.org/abs/1409.4842"&gt;http://arxiv.org/abs/1409.4842&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1409.4842"&gt;arXiv:1409.4842&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/SzegedyLJSRAEVR14-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1803-03642'&gt;Abhinav Valada, Noha Radwan, and Wolfram Burgard.
Deep auxiliary learning for visual localization and odometry.
&lt;em&gt;CoRR&lt;/em&gt;, 2018.
URL: &lt;a href="http://arxiv.org/abs/1803.03642"&gt;http://arxiv.org/abs/1803.03642&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1803.03642"&gt;arXiv:1803.03642&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1803-03642-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/VaswaniSPUJGKP17'&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&amp;nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1706.03762"&gt;http://arxiv.org/abs/1706.03762&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1706.03762"&gt;arXiv:1706.03762&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/VaswaniSPUJGKP17-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='DBLP:journals/corr/abs-1709-08429'&gt;Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni.
Deepvo: towards end-to-end visual odometry with deep recurrent convolutional neural networks.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1709.08429"&gt;http://arxiv.org/abs/1709.08429&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1709.08429"&gt;arXiv:1709.08429&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/abs-1709-08429-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='wei2020deepsfm'&gt;Xingkui Wei, Yinda Zhang, Zhuwen Li, Yanwei Fu, and Xiangyang Xue.
Deepsfm: structure from motion via deep bundle adjustment.
2020.
&lt;a href="https://arxiv.org/abs/1912.09697"&gt;arXiv:1912.09697&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-wei2020deepsfm-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='GalvezLopez2012'&gt;D.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Galvez-López&lt;/span&gt; and J.&amp;nbsp;D. &lt;span class="bibtex-protected"&gt;Tardos&lt;/span&gt;.
Bags of binary words for fast place recognition in image sequences.
&lt;em&gt;IEEE Transactions on Robotics&lt;/em&gt;, 28(5):1188&amp;ndash;1197, 2012. &lt;a class="cite-backref" href="#ref-GalvezLopez2012-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='1288525'&gt;D.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Nister&lt;/span&gt;.
An efficient solution to the five-point relative pose problem.
&lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/em&gt;, 26(6):756&amp;ndash;770, 2004.
&lt;a href="https://doi.org/10.1109/TPAMI.2004.17"&gt;doi:10.1109/TPAMI.2004.17&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-1288525-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='6126544'&gt;E.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Rublee&lt;/span&gt;, V.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Rabaud&lt;/span&gt;, K.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Konolige&lt;/span&gt;, and G.&amp;nbsp;&lt;span class="bibtex-protected"&gt;Bradski&lt;/span&gt;.
Orb: an efficient alternative to sift or surf.
In &lt;em&gt;2011 International Conference on Computer Vision&lt;/em&gt;, volume, 2564&amp;ndash;2571. 2011.
&lt;a href="https://doi.org/10.1109/ICCV.2011.6126544"&gt;doi:10.1109/ICCV.2011.6126544&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-6126544-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="Machine learning"></category><category term="deep-learning"></category><category term="computer-vision"></category></entry><entry><title>Fast uncertainty estimation for mobilenet.</title><link href="https://hoff97.github.io/develop/mobilenet-uncertainty.html" rel="alternate"></link><published>2020-08-31T16:25:00+02:00</published><updated>2020-08-31T16:25:00+02:00</updated><author><name>Frithjof Winkelmann</name></author><id>tag:hoff97.github.io,2020-08-31:/develop/mobilenet-uncertainty.html</id><summary type="html">&lt;p&gt;Here I explore how uncertainty estimation can be done very quickly with mobilenet&lt;/p&gt;</summary><content type="html">&lt;p&gt;While writing my bachelor thesis, I often ended up using &lt;a href="http://detexify.kirelabs.org/classify.html"&gt;Detexify&lt;/a&gt;
to look up latex symbols I forgot. Unfortunately, mobile data doesnt work very
well in germany, so when I was traveling a lot of times I could'nt access this page.
For this reason, I build &lt;a href="https://detext.haskai.de/client/"&gt;Detext&lt;/a&gt;,
a progressive web app (PWA), that classifies latex symbols without needing
a internet connection. It uses MobileNet &lt;a href='#Howard2017' id='ref-Howard2017-1'&gt;(Howard et al., 2017)&lt;/a&gt;,
which is run directly on the client side using &lt;a href="https://github.com/microsoft/onnxjs"&gt;onnx.js&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/detext_example.png" title="Example of detext predicting the latex code for the alpha character"&gt;&lt;/p&gt;
&lt;p&gt;This works pretty well for most symbols. The only pet peeve I had with
it until recently was, that it wont tell you when its not
certain about the predicted symbol or simply predicts the wrong symbol.
In the following example, the app predicts the &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; (\rightarrow) symbol,
since it does'nt know &lt;span class="math"&gt;\(\rightharpoonup\)&lt;/span&gt; (\rightharpoonup).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/detext_wrong.png" title="Detext predicts \rightarrow since it doesnt know \rightharpoonup"&gt;&lt;/p&gt;
&lt;p&gt;Ideally of course, the prediction would include some uncertainty of the network.
This is known as uncertainty estimation and a variety of approaches exist to
solve it, for example using model ensembles &lt;a href='#Lakshminarayanan2017' id='ref-Lakshminarayanan2017-1'&gt;(Lakshminarayanan et al., 2017)&lt;/a&gt;, predicting a uncertainty &lt;a href='#Sequ2019' id='ref-Sequ2019-1'&gt;(Segu et al., 2019)&lt;/a&gt; or
using test time dropout &lt;a href='#Gal2016' id='ref-Gal2016-1'&gt;(Gal and Ghahramani, 2016)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article I will focus on test time dropout. Normally, dropout is
only enabled during training and is used to prevent overfitting.
To enable the model uncertainty, instead of disabling the dropout layers(s)
during testing, one can instead keep it enabled. Feeding in the same input
to a network with test time dropout will then give different outputs,
depending on which neurons stay enabled.&lt;/p&gt;
&lt;p&gt;Lets look at an example. The top-5 softmax values that mobilenet predicts
for the following picture
&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/rightharpoon.png" title="Hand drawn right harpoon symbol"&gt;&lt;/p&gt;
&lt;p&gt;are shown in the plot below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/wrong_softmax.png" title="Softmax of the top-5 classes"&gt;&lt;/p&gt;
&lt;p&gt;Since the &lt;span class="math"&gt;\(\rightharpoonup\)&lt;/span&gt; symbol is not in the training dataset, these
are all wrong of course. Unfortunately, the softmax score for the top class
is pretty high, so it can not be used as an uncertainty estimate.&lt;/p&gt;
&lt;p&gt;To estimate the model uncertainty, we turn on dropout at test time
and look at the model predictions. For this we pass the same image
through the network 100 times and visualize the predictions together with
their variance:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/rightharpoonerr.png" title="Softmax of the top-5 classes with error bars"&gt;&lt;/p&gt;
&lt;p&gt;Here the black bars indicate the standard deviation of the predicted
softmax values. We can see that using test time dropout induces some variance.&lt;/p&gt;
&lt;p&gt;If we compare this to the prediction of a symbol that is included in the
dataset, like the following&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hand drawn right arrow symbol" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/rightarrow.png" title="Hand drawn right arrow symbol"&gt;&lt;/p&gt;
&lt;p&gt;we can see that this has a lower variance:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax of the top-5 classes with error bars" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/rightarrowerr.png" title="Softmax of the top-5 classes with error bars"&gt;.&lt;/p&gt;
&lt;p&gt;Lets look at a few more examples. Here are the variances of symbols that are close to the training set:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/uncertain_good.png" title="Variances of images similar to the training set"&gt;.&lt;/p&gt;
&lt;p&gt;And here those of pictures that are not found in the training set:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/uncertain_bad.png" title="Variances of images not similar to the training set"&gt;.&lt;/p&gt;
&lt;p&gt;This is nice, since it allows us to detect when the model is uncertain about its
predictions. Unfortunately, this approach requires us to make multiple
forward passes through the model.
In the case of the Detext app this is not acceptable. The app should run on
mobile devices too, and there a forward pass might already take ~1 second.
Doing 10 forward passes just to estimate the uncertainty would take too long.
Fortunately the architecture of mobilenet allows us to estimate the variance
with only one forward pass.&lt;/p&gt;
&lt;p&gt;The standard implementation of MobileNet only includes a single dropout layer.
It computes a image feature with the feature network
&lt;span class="math"&gt;\(f: \mathbb{R}^{W \times H} \rightarrow \mathbb{R}^D\)&lt;/span&gt;, then applies dropout
and then used a single fully connected layer and a softmax, so in full the prediction
of an image is
&lt;/p&gt;
&lt;div class="math"&gt;$$ pred = softmax(W \times dropout(f(image)) $$&lt;/div&gt;
&lt;p&gt;The feature of an image will thus always be the same, even with test time dropout
enabled. We can use a few simple tricks to estimate the variance.&lt;/p&gt;
&lt;p&gt;The covariance matrix &lt;span class="math"&gt;\(\Sigma^F\)&lt;/span&gt; of &lt;span class="math"&gt;\(F = dropout(f(image))\)&lt;/span&gt; is simply a diagonal matrix
with &lt;span class="math"&gt;\(\Sigma^F_{ii} = p/(1-p)*f(image)_i^2\)&lt;/span&gt; (its just a bernoully random vector
multiplied by &lt;span class="math"&gt;\(f(image)/(1-p)\)&lt;/span&gt; ).&lt;/p&gt;
&lt;p&gt;We get the covariance after the linear layer by using the fact that
&lt;span class="math"&gt;\(X = W F\)&lt;/span&gt; has covariance &lt;span class="math"&gt;\(W \Sigma^F W^T\)&lt;/span&gt;.
Now we only have to find out how to compute
the variance of &lt;span class="math"&gt;\(softmax(X)\)&lt;/span&gt; given the covariance and mean of &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can do this using the
&lt;a href="https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables"&gt;taylor expansion for the moments of random variables&lt;/a&gt;. For the covariance
this reads:&lt;/p&gt;
&lt;div class="math"&gt;$$ Cov(f(X)) \approx J_f \Sigma^X J_f^T$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(J_f\)&lt;/span&gt; is the jacobian of &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluated at &lt;span class="math"&gt;\(E[X]\)&lt;/span&gt;.
We only need to find the jacobian of the softmax which is given by:&lt;/p&gt;
&lt;div class="math"&gt;$$ J^{softmax}_{ij}(X) = \begin{cases}
    softmax(X)_i (1 - softmax(X)_i) &amp;amp; i=j\\
    -softmax(X)_j softmax(X)_i &amp;amp; i \neq j
\end{cases} $$&lt;/div&gt;
&lt;p&gt;In total the evaluation of the variance is done by&lt;/p&gt;
&lt;div class="math"&gt;$$
F = f(image)\\
\Sigma^F_{ii} = \tfrac{p}{1-p}*F_i^2\\
X = W F\\
\Sigma^X = W \Sigma^F W^T\\
S = softmax(X)\\
\Sigma^S = J^{softmax}(X) \Sigma^X J^{softmax}(X)^T
$$&lt;/div&gt;
&lt;p&gt;Lets compare the variance we get with this estimate to the variance estimated from sampling:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/variance_comparison.png" title="Comparison of variance estimated from sampling and approximation"&gt;&lt;/p&gt;
&lt;p&gt;We can see that the approximated variance is not exactly equal to the variance estimated from sampling, but it is generally
higher for images that are not similar from the training set.&lt;/p&gt;
&lt;p&gt;This can be used as a rough indicator, when the prediction of MobileNet might not be trusted.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://detext.haskai.de/client/"&gt;Detext&lt;/a&gt; app this looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://hoff97.github.io/develop/images/mobilenet_uncertainty/detext_uncertain.png" title="Example of uncertainty estimate in detext"&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='Gal2016'&gt;Yarin Gal and Zoubin Ghahramani.
Dropout as a bayesian approximation: representing model uncertainty in deep learning.
In &lt;em&gt;Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48&lt;/em&gt;, ICML'16, 1050&amp;ndash;1059. JMLR.org, 2016.
URL: &lt;a href="https://dl.acm.org/doi/10.5555/3045390.3045502"&gt;https://dl.acm.org/doi/10.5555/3045390.3045502&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Gal2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Howard2017'&gt;Andrew&amp;nbsp;G. Howard, Menglong Zhu, Bo&amp;nbsp;Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: efficient convolutional neural networks for mobile vision applications.
&lt;em&gt;CoRR&lt;/em&gt;, 2017.
URL: &lt;a href="http://arxiv.org/abs/1704.04861"&gt;http://arxiv.org/abs/1704.04861&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1704.04861"&gt;arXiv:1704.04861&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Howard2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Lakshminarayanan2017'&gt;Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
Simple and scalable predictive uncertainty estimation using deep ensembles.
In &lt;em&gt;Proceedings of the 31st International Conference on Neural Information Processing Systems&lt;/em&gt;, NIPS'17, 6405&amp;ndash;6416. Red Hook, NY, USA, 2017. Curran Associates Inc.
URL: &lt;a href="https://dl.acm.org/doi/10.5555/3295222.3295387"&gt;https://dl.acm.org/doi/10.5555/3295222.3295387&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Lakshminarayanan2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='Sequ2019'&gt;Mattia Seg&lt;span class="bibtex-protected"&gt;&lt;span class="bibtex-protected"&gt;ù&lt;/span&gt;&lt;/span&gt;, Antonio Loquercio, and Davide Scaramuzza.
A general framework for uncertainty estimation in deep learning.
&lt;em&gt;CoRR&lt;/em&gt;, 2019.
URL: &lt;a href="http://arxiv.org/abs/1907.06890"&gt;http://arxiv.org/abs/1907.06890&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1907.06890"&gt;arXiv:1907.06890&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-Sequ2019-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="Machine learning"></category><category term="deep-learning"></category><category term="uncertainty-estiamtion"></category></entry></feed>